{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6c8b0f-e21b-401f-ae0c-7b426e6cfb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb distribution: Counter({'Write': 165, 'Explain': 134, 'Implement': 111, 'Set': 88, 'Create': 71, 'Optimize': 62, 'Compare': 48, 'Debug': 45, 'Develop': 40, 'Handle': 34, 'Use': 34, 'Describe': 31, 'Containerize': 19, 'Validate': 18, 'Merge': 17, 'Join': 16, 'Parse': 15})\n",
      "Original dataset size: 49, Augmented dataset size: 948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saiet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load base dataset\n",
    "def load_base_dataset(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return [(item['original'], item['enhanced']) for item in data]\n",
    "\n",
    "# Augmentation function\n",
    "def augment_data(data, augment_factor=20):\n",
    "    augmented_data = data.copy()\n",
    "    \n",
    "    # Expanded synonym dictionary\n",
    "    synonyms = {\n",
    "        'explain': ['describe', 'clarify', 'elucidate', 'illustrate', 'detail'],\n",
    "        'write': ['create', 'code', 'implement', 'develop', 'build'],\n",
    "        'provide': ['give', 'supply', 'offer', 'present', 'deliver'],\n",
    "        'how': ['ways to', 'method to', 'approach to', 'technique for', 'steps to'],\n",
    "        'implement': ['build', 'develop', 'code', 'construct', 'execute'],\n",
    "        'debug': ['troubleshoot', 'fix', 'resolve', 'diagnose', 'repair']\n",
    "    }\n",
    "    \n",
    "    # Ensure balanced verb usage\n",
    "    verbs = ['Explain', 'Write', 'Debug', 'Create', 'Implement', 'Describe', 'Develop', 'Set up', 'Optimize', 'Compare']\n",
    "    \n",
    "    for _ in range(augment_factor):\n",
    "        for original, enhanced in data:\n",
    "            new_original, new_enhanced = original, enhanced\n",
    "            \n",
    "            # Synonym Replacement\n",
    "            for word, syn_list in synonyms.items():\n",
    "                if random.random() < 0.4:\n",
    "                    new_original = new_original.replace(word, random.choice(syn_list))\n",
    "                    new_enhanced = new_enhanced.replace(word, random.choice(syn_list))\n",
    "            \n",
    "            # Paraphrasing\n",
    "            if random.random() < 0.5:\n",
    "                prefixes = ['Can you', 'I need', 'Please tell me how to', 'How do I', 'Show me how to', 'What is the way to']\n",
    "                for prefix in prefixes:\n",
    "                    if new_original.startswith(prefix):\n",
    "                        new_prefix = random.choice([p for p in prefixes if p != prefix])\n",
    "                        new_original = new_original.replace(prefix, new_prefix, 1)\n",
    "                        break\n",
    "            \n",
    "            # Filler Word Addition/Removal\n",
    "            fillers = ['please', 'kindly', 'in detail', 'comprehensive', 'step-by-step', 'quickly']\n",
    "            if random.random() < 0.5:\n",
    "                words = new_original.split()\n",
    "                insert_pos = random.randint(1, len(words))\n",
    "                filler = random.choice(fillers)\n",
    "                new_original = ' '.join(words[:insert_pos] + [filler] + words[insert_pos:])\n",
    "            else:\n",
    "                for filler in fillers:\n",
    "                    new_original = new_original.replace(filler, '').strip()\n",
    "            \n",
    "            # Verb Variation (balanced)\n",
    "            if random.random() < 0.4:\n",
    "                for verb in verbs:\n",
    "                    if new_enhanced.startswith(verb):\n",
    "                        new_verb = random.choice([v for v in verbs if v != verb])\n",
    "                        new_enhanced = new_enhanced.replace(verb, new_verb, 1)\n",
    "                        break\n",
    "            \n",
    "            # Random Cropping\n",
    "            if random.random() < 0.3 and len(new_original.split()) > 5:\n",
    "                words = new_original.split()\n",
    "                start = random.randint(0, 2)\n",
    "                end = len(words) - random.randint(0, 2)\n",
    "                new_original = ' '.join(words[start:end])\n",
    "            \n",
    "            # Random Word Shuffling\n",
    "            if random.random() < 0.2:\n",
    "                words = new_original.split()\n",
    "                if len(words) > 3:\n",
    "                    i, j = random.sample(range(len(words)), 2)\n",
    "                    words[i], words[j] = words[j], words[i]\n",
    "                    new_original = ' '.join(words)\n",
    "            \n",
    "            # Add augmented pair if different\n",
    "            if (new_original, new_enhanced) != (original, enhanced):\n",
    "                augmented_data.append((new_original, new_enhanced))\n",
    "    \n",
    "    # Balance verbs in enhanced prompts\n",
    "    from collections import Counter\n",
    "    verb_counts = Counter(pair[1].split()[0] for pair in augmented_data)\n",
    "    print(f\"Verb distribution: {verb_counts}\")\n",
    "    \n",
    "    return augmented_data[:1000]  # Limit to 1,000 pairs\n",
    "\n",
    "# Save dataset\n",
    "def save_dataset(data, file_path='large_programming_prompts.json'):\n",
    "    json_data = [{'original': orig, 'enhanced': enh} for orig, enh in data]\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    # Load base dataset\n",
    "    data = load_base_dataset('base_programming_prompts.json')\n",
    "    \n",
    "    # Augment to ~1,000 pairs\n",
    "    augmented_data = augment_data(data, augment_factor=20)\n",
    "    \n",
    "    # Save augmented dataset\n",
    "    save_dataset(augmented_data)\n",
    "    print(f\"Original dataset size: {len(data)}, Augmented dataset size: {len(augmented_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15335ce1-cba5-4e5f-ba59-48cea0bcd5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
