{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31391057-82a3-4569-b64e-05ccefc93ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6854a63-9dfc-4401-8b89-9a54d4685298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75b83043-66b2-4474-ba85-b456a6230010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compose', 'drop a line', 'indite', 'pen', 'publish', 'save', 'spell', 'write']\n"
     ]
    }
   ],
   "source": [
    "import synonyms\n",
    "print(synonyms.get_all_synonyms(\"write\", pos=synonyms.wordnet.VERB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93d8fde7-7b3b-41d3-b16c-a719538f1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_verbs_from_dataset(data):\n",
    "    verbs = set()\n",
    "    for original, enhanced in data:\n",
    "        for text in [original, enhanced]:\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            for token in tokens:\n",
    "                if is_verb(token):\n",
    "                    verbs.add(token)\n",
    "    return sorted(list(verbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a16a767-fda1-4c44-b683-6210469d7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_verb(word):\n",
    "    synsets = wordnet.synsets(word, pos=wordnet.VERB)\n",
    "    return len(synsets) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43e3cc9d-c64c-4971-b62f-b2096c7ed2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(data, augment_factor=10):\n",
    "    augmented_data = data.copy()\n",
    "    \n",
    "    # Expanded synonym dictionary\n",
    "    verbs = extract_verbs_from_dataset(augmented_data)\n",
    "    \n",
    "    synonyms = {}\n",
    "    for verb in verbs:\n",
    "        synonyms[verb] = synonyms.get_all_synonyms(verb, pos=synonyms.wordnet.VERB)\n",
    "    print(synonyms)\n",
    "    \n",
    "    for _ in range(augment_factor):\n",
    "        for original, enhanced in data:\n",
    "            new_original, new_enhanced = original, enhanced\n",
    "            \n",
    "            # Synonym Replacement\n",
    "            for word, syn_list in synonyms.items():\n",
    "                if random.random() < 0.4:  # 40% chance to replace\n",
    "                    new_original = new_original.replace(word, random.choice(syn_list))\n",
    "                    new_enhanced = new_enhanced.replace(word, random.choice(syn_list))\n",
    "            \n",
    "            # Paraphrasing\n",
    "            if random.random() < 0.5:\n",
    "                if new_original.startswith('Can you'):\n",
    "                    new_original = new_original.replace('Can you', random.choice(['How do I', 'Show me how to', 'What is the way to']), 1)\n",
    "                elif new_original.startswith('I need'):\n",
    "                    new_original = new_original.replace('I need', random.choice(['Show me how to', 'I want to know how to']), 1)\n",
    "            \n",
    "            # Filler Word Addition/Removal\n",
    "            fillers = ['please', 'kindly', 'in detail', 'comprehensive', 'step-by-step']\n",
    "            if random.random() < 0.5:\n",
    "                words = new_original.split()\n",
    "                insert_pos = random.randint(1, len(words))\n",
    "                filler = random.choice(fillers)\n",
    "                new_original = ' '.join(words[:insert_pos] + [filler] + words[insert_pos:])\n",
    "            else:\n",
    "                for filler in fillers:\n",
    "                    new_original = new_original.replace(filler, '').strip()\n",
    "            \n",
    "            # Verb Variation (for enhanced)\n",
    "            verbs = ['Explain', 'Write', 'Debug', 'Create', 'Implement', 'Describe', 'Develop']\n",
    "            if random.random() < 0.4:\n",
    "                for verb in verbs:\n",
    "                    if new_enhanced.startswith(verb):\n",
    "                        new_enhanced = new_enhanced.replace(verb, random.choice(verbs), 1)\n",
    "                        break\n",
    "            \n",
    "            # Random Cropping\n",
    "            if random.random() < 0.3 and len(new_original.split()) > 5:\n",
    "                words = new_original.split()\n",
    "                start = random.randint(0, 2)\n",
    "                end = len(words) - random.randint(0, 2)\n",
    "                new_original = ' '.join(words[start:end])\n",
    "            \n",
    "            # Random Word Shuffling (add noise)\n",
    "            if random.random() < 0.2:\n",
    "                words = new_original.split()\n",
    "                if len(words) > 3:\n",
    "                    i, j = random.sample(range(len(words)), 2)\n",
    "                    words[i], words[j] = words[j], words[i]\n",
    "                    new_original = ' '.join(words)\n",
    "            \n",
    "            # Add augmented pair if different\n",
    "            if (new_original, new_enhanced) != (original, enhanced):\n",
    "                augmented_data.append((new_original, new_enhanced))\n",
    "    \n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4585dafa-a675-4f16-bfe2-3ce2ef1ce9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return [(item['original'], item['enhanced']) for item in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b0786f8-9bac-49b8-b303-92f37fcfdee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "    word_count = {}\n",
    "    for text in texts:\n",
    "        for word in word_tokenize(text.lower()):\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "    for word, count in word_count.items():\n",
    "        if count >= 1:  # Include all words (small dataset)\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab, {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10987c12-5d87-4823-a826-7cb54a40aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_augmented_dataset(data, file_path='augmented_programming_prompts.json'):\n",
    "    json_data = [{'original': orig, 'enhanced': enh} for orig, enh in data]\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f30efc2-e473-4321-911c-d36bc8ee8960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_convert(text, vocab, max_len):\n",
    "    tokens = word_tokenize(text.lower())[:max_len-1]\n",
    "    token_ids = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "    token_ids = [vocab['<SOS>']] + token_ids + [vocab['<EOS>']]\n",
    "    if len(token_ids) < max_len:\n",
    "        token_ids += [vocab['<PAD>']] * (max_len - len(token_ids))\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f851b5d-c57f-4423-9ae1-1af3fccfcab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDataset(Dataset):\n",
    "    def __init__(self, data, vocab, max_len=50):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.data[idx]\n",
    "        src_ids = tokenize_and_convert(src, self.vocab, self.max_len)\n",
    "        tgt_ids = tokenize_and_convert(tgt, self.vocab, self.max_len)\n",
    "        return torch.tensor(src_ids), torch.tensor(tgt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "468cc5f3-d1ea-4a14-8ffa-7d464c66a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=128, hidden_size=256):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.encoder = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        vocab_size = self.fc.out_features\n",
    "        \n",
    "        # Encoder\n",
    "        embedded = self.embedding(src)\n",
    "        enc_output, (hidden, cell) = self.encoder(embedded)\n",
    "        \n",
    "        # Decoder\n",
    "        outputs = torch.zeros(batch_size, tgt_len-1, vocab_size).to(src.device)\n",
    "        dec_input = tgt[:, 0].unsqueeze(1)  # Start with <SOS>\n",
    "        dec_hidden = (hidden, cell)\n",
    "        \n",
    "        for t in range(tgt_len-1):\n",
    "            dec_embed = self.embedding(dec_input)\n",
    "            dec_output, dec_hidden = self.decoder(dec_embed, dec_hidden)\n",
    "            output = self.fc(dec_output.squeeze(1))\n",
    "            outputs[:, t, :] = output\n",
    "            \n",
    "            # Teacher forcing\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                dec_input = tgt[:, t+1].unsqueeze(1)\n",
    "            else:\n",
    "                dec_input = output.argmax(1).unsqueeze(1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d84e14d-e660-49c7-b73e-1498fbf632cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=30, lr=0.001, device='cpu'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore <PAD>\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Anneal teacher forcing ratio\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        tf_ratio = max(0.1, 0.5 - (0.4 * epoch / epochs))\n",
    "        \n",
    "        for src, tgt in train_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt, teacher_forcing_ratio=tf_ratio)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), tgt[:, 1:].contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in val_loader:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
    "                loss = criterion(output.view(-1, output.size(-1)), tgt[:, 1:].contiguous().view(-1))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Log sample output\n",
    "        if epoch % 5 == 0:\n",
    "            sample_src, _ = val_loader.dataset[0]\n",
    "            sample_prompt = ' '.join([inv_vocab.get(t.item(), '<UNK>') for t in sample_src if t.item() not in [0, 1, 2]])\n",
    "            enhanced = enhance_prompt(model, sample_prompt, vocab, inv_vocab, device=device)\n",
    "            print(f'Epoch {epoch+1}, Sample Input: {sample_prompt}')\n",
    "            print(f'Epoch {epoch+1}, Sample Output: {enhanced}')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}, TF Ratio: {tf_ratio:.2f}')\n",
    "    \n",
    "    torch.save(model.state_dict(), 'prompt_enhancer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f287f494-0e9f-4859-bd11-4a9b6de38e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_prompt(model, prompt, vocab, inv_vocab, max_len=50, device='cpu'):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    token_ids = tokenize_and_convert(prompt, vocab, max_len)\n",
    "    src = torch.tensor([token_ids]).to(device)\n",
    "    \n",
    "    # Encoder\n",
    "    embedded = model.embedding(src)\n",
    "    _, (hidden, cell) = model.encoder(embedded)\n",
    "    \n",
    "    # Decoder\n",
    "    dec_input = torch.tensor([[vocab['<SOS>']]]).to(device)\n",
    "    output_tokens = []\n",
    "    for _ in range(max_len):\n",
    "        dec_embed = model.embedding(dec_input)\n",
    "        dec_output, (hidden, cell) = model.decoder(dec_embed, (hidden, cell))\n",
    "        output = model.fc(dec_output.squeeze(1))\n",
    "        pred_token = output.argmax(1).item()\n",
    "        if pred_token == vocab['<EOS>']:\n",
    "            break\n",
    "        output_tokens.append(pred_token)\n",
    "        dec_input = torch.tensor([[pred_token]]).to(device)\n",
    "    \n",
    "    return ' '.join(inv_vocab.get(t, '<UNK>') for t in output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d5f9999-1382-4254-be07-ef1d4fc25748",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('large_programming_prompts.json')\n",
    "augmented_data = augment_data(data, augment_factor=5)\n",
    "save_augmented_dataset(augmented_data)\n",
    "print(f\"Original dataset size: {len(data)}, Augmented dataset size: {len(augmented_data)}\")\n",
    "#random.shuffle(augmented_data)\n",
    "train_size = int(0.8 * len(augmented_data))\n",
    "train_data, val_data = augmented_data[:train_size], augmented_data[train_size:]\n",
    "all_texts = [pair[0] for pair in augmented_data] + [pair[1] for pair in augmented_data]\n",
    "vocab, inv_vocab = build_vocab(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59111a57-6ede-4072-9ae8-880fbe534698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actions',\n",
       " 'approach',\n",
       " 'array',\n",
       " 'arrays',\n",
       " 'automate',\n",
       " 'bash',\n",
       " 'best',\n",
       " 'build',\n",
       " 'builded',\n",
       " 'calculate',\n",
       " 'can',\n",
       " 'check',\n",
       " 'clarify',\n",
       " 'closures',\n",
       " 'code',\n",
       " 'codeed',\n",
       " 'compare',\n",
       " 'connect',\n",
       " 'connecting',\n",
       " 'construct',\n",
       " 'constructed',\n",
       " 'containerize',\n",
       " 'control',\n",
       " 'create',\n",
       " 'debug',\n",
       " 'deliver',\n",
       " 'describe',\n",
       " 'detail',\n",
       " 'detailed',\n",
       " 'develop',\n",
       " 'developed',\n",
       " 'do',\n",
       " 'elucidate',\n",
       " 'email',\n",
       " 'execute',\n",
       " 'executeed',\n",
       " 'explain',\n",
       " 'fault',\n",
       " 'fetch',\n",
       " 'figure',\n",
       " 'file',\n",
       " 'files',\n",
       " 'find',\n",
       " 'fix',\n",
       " 'function',\n",
       " 'functions',\n",
       " 'game',\n",
       " 'give',\n",
       " 'go',\n",
       " 'guide',\n",
       " 'handle',\n",
       " 'hash',\n",
       " 'help',\n",
       " 'illustrate',\n",
       " 'implement',\n",
       " 'implemented',\n",
       " 'is',\n",
       " 'join',\n",
       " 'know',\n",
       " 'learn',\n",
       " 'linked',\n",
       " 'list',\n",
       " 'looking',\n",
       " 'merge',\n",
       " 'need',\n",
       " 'number',\n",
       " 'offer',\n",
       " 'optimize',\n",
       " 'out',\n",
       " 'parse',\n",
       " 'perform',\n",
       " 'phone',\n",
       " 'please',\n",
       " 'present',\n",
       " 'process',\n",
       " 'processing',\n",
       " 'program',\n",
       " 'programming',\n",
       " 'project',\n",
       " 'promises',\n",
       " 'provide',\n",
       " 'query',\n",
       " 'queue',\n",
       " 'repair',\n",
       " 'resolve',\n",
       " 'rest',\n",
       " 'reverse',\n",
       " 'scrape',\n",
       " 'scraping',\n",
       " 'script',\n",
       " 'search',\n",
       " 'sequence',\n",
       " 'set',\n",
       " 'setting',\n",
       " 'show',\n",
       " 'solve',\n",
       " 'sort',\n",
       " 'sorted',\n",
       " 'sorting',\n",
       " 'stack',\n",
       " 'steps',\n",
       " 'string',\n",
       " 'structure',\n",
       " 'structures',\n",
       " 'struggling',\n",
       " 'supply',\n",
       " 'supposed',\n",
       " 'sways',\n",
       " 'table',\n",
       " 'tables',\n",
       " 'tell',\n",
       " 'tests',\n",
       " 'troubleshoot',\n",
       " 'trying',\n",
       " 'understand',\n",
       " 'understanding',\n",
       " 'up',\n",
       " 'uploads',\n",
       " 'use',\n",
       " 'using',\n",
       " 'validate',\n",
       " 'want',\n",
       " 'web',\n",
       " 'write',\n",
       " 'writing',\n",
       " 'wrong']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_verbs_from_dataset(augmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c515163e-2082-472c-9c9f-cd882b43bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PromptDataset(train_data, vocab)\n",
    "val_dataset = PromptDataset(val_data, vocab)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71d054ae-29f9-4723-b5f9-c71e9939783a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Sample Input: supply a to example for connecting kindly code a mysql database using php\n",
      "Epoch 1, Sample Output: write python function to check palindrome string\n",
      "Epoch 1, Train Loss: 2.2964, Val Loss: 3.7987, TF Ratio: 0.50\n",
      "Epoch 2, Train Loss: 1.6544, Val Loss: 3.4545, TF Ratio: 0.49\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Seq2Seq(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vocab), embed_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m train_model(model, train_loader, val_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, epochs, lr, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m output \u001b[38;5;241m=\u001b[39m model(src, tgt, teacher_forcing_ratio\u001b[38;5;241m=\u001b[39mtf_ratio)\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), tgt[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)  \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Seq2Seq(vocab_size=len(vocab), embed_size=128, hidden_size=256)\n",
    "\n",
    "# Train model\n",
    "train_model(model, train_loader, val_loader, epochs=30, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c1c25-fde9-41ba-88ea-ef30dcbc9522",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"bro i need binary search implementation\"\n",
    "enhanced = enhance_prompt(model, test_prompt, vocab, inv_vocab, device=device)\n",
    "print(f\"Original: {test_prompt}\")\n",
    "print(f\"Enhanced: {enhanced}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c9f4d-1ccf-491e-8138-b79485cafc05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
